<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Gamer Bao</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Gamer Bao">
<meta property="og:url" content="http://bcyyu.wang/index.html">
<meta property="og:site_name" content="Gamer Bao">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gamer Bao">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="Gamer Bao" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/icon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/img/icon.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Gamer Bao</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
							<li><a href="/tags/随笔">随笔</a></li>
				        
							<li><a href="/tags/Hadoop">Hadoop</a></li>
				        
							<li><a href="/tags/mysql">Mysql</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/bao2313120/" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="http://weibo.com/1805575780/profile?rightmod=1&wvr=6&mod=personinfo" title="weibo">weibo</a>
					        
								<a class="rss" target="_blank" href="/#" title="rss">rss</a>
					        
								<a class="zhihu" target="_blank" href="http://www.zhihu.com/people/xu-zheng-yang-27" title="zhihu">zhihu</a>
					        
								<a class="mail" target="_blank" href="/391514467@qq.com" title="mail">mail</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/HDFS/" style="font-size: 20px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 20px;">Hadoop</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://blog.jamespan.me/">潘家邦</a>
			        
			        </div>
				</section>
				

				
				
				<section class="switch-part switch-part4">
				
					<div id="js-aboutme">游戏创业者，坚持做自己！</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Gamer Bao</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="/img/icon.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">Gamer Bao</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
					<li><a href="/tags/随笔">随笔</a></li>
		        
					<li><a href="/tags/Hadoop">Hadoop</a></li>
		        
					<li><a href="/tags/mysql">Mysql</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/bao2313120/" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/1805575780/profile?rightmod=1&wvr=6&mod=personinfo" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="/#" title="rss">rss</a>
			        
						<a class="zhihu" target="_blank" href="http://www.zhihu.com/people/xu-zheng-yang-27" title="zhihu">zhihu</a>
			        
						<a class="mail" target="_blank" href="/391514467@qq.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-HDFS架构设计要点" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/05/HDFS架构设计要点/" class="article-date">
  	<time datetime="2015-11-05T11:14:14.000Z" itemprop="datePublished">2015-11-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/05/HDFS架构设计要点/">HDFS架构设计要点</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="前提和设计目标">前提和设计目标</h4><ul>
<li>硬件错误是常态，而非异常情况，HDFS可能是有成百上千的server组成，任何一个组件都有可能一直失效，因此错误检测和快速、自动的恢复是 HDFS的核心架构目标。</li>
<li>跑在HDFS上的应用与一般的应用不同，它们主要是以流式读为主，做批量处理；比之关注数据访问的低延迟问题 ，更关键的在于数据访问的高吞吐量。</li>
<li>HDFS以支持大数据集合为目标，一个存储在上面的典型文件大小一般都在千兆至T字节，一个单一HDFS实例应该能支撑数以千万计的文件。</li>
<li>HDFS应用对文件要求的是 write-one-read-many访问模型。一个文件经过创建、写、关闭之后就不需要改变。这一假设简化了数据一致性问题，使高吞吐量的数据访问成为可能。典型的如MapReduce框架，或者一个web crawler应用都很适合这个模型。</li>
<li>移动计算的代价比之移动数据的代价低。一个应用请求的计算，离它操作的数据越近就越高效，这在数据达到海量级别的时候更是如此。将计算移动到数据附近，比之将数据移动到应用所在显然更好，HDFS提供给应用这样的接口。</li>
<li>在异构的软硬件平台间的可移植性。</li>
</ul>
<h4 id="Namenode和_Datanode">Namenode和 Datanode</h4><p>HDFS采用master/slave架构。一个HDFS集群是有一个Namenode和一定数目的Datanode组成。Namenode是一个中心服务器，负责管理文件系统的namespace和客户端对文件的访问。 Datanode在集群中一般是一个节点一个，负责管理节点上它们附带的存储。在内部，一个文件其实分成一个或多个block，这些block存储在Datanode集合里。 Namenode执行文件系统的 namespace操作，例如 打开、关闭、重命名文件和目录，同时决定block到具体Datanode节点的映射。 Datanode在Namenode的指挥下进行block的创建、删除和复制。 Namenode和Datanode都是设计成可以跑在普通的廉价的运行linux的机器上。HDFS采用java语言开发，因此可以部署在很大范围的机器上。一个典型的部署场景是一台机器跑一个单独的Namenode节点，集群中的其他机器各跑一个Datanode实例。这个架构并不排除一台机器上跑多个Datanode，不过这比较少见。<br>单一节点的Namenode大大简化了系统的架构。Namenode负责保管和管理所有的HDFS元数据，因而用户数据就不需要通过Namenode（也就是说文件数据的读写是直接在Datanode上）。</p>
<h4 id="文件系统的_namespace">文件系统的 namespace</h4><p>   HDFS支持传统的层次型文件组织，与大多数其他文件系统类似，用户可以创建目录，并在其间创建、删除、移动和重命名文件。 HDFS不支持 user quotas和访问权限，也不支持链接（ link)，不过当前的架构并不排除实现这些特性。Namenode维护文件系统的namespace，任何对文件系统namespace和文件属性的修改都将被Namenode记录下来。应用可以设置 HDFS保存的文件的副本数目，文件副本的数目称为文件的replication 因子，这个信息也是由Namenode保存。</p>
<h4 id="数据复制">数据复制</h4><p>   HDFS被设计成在一个大集群中可以跨机器地可靠地存储海量的文件。它将每个文件存储成 block序列，除了最后一个 block，所有的 block都是同 样的大小。文件的所有 block为了容错都会被复制。每个文件的 block大小和 replication因子都是可配置的。 Replication因子可 以在文件创建的时候配置，以后也可以改变。 HDFS中的文件是 write-one，并且严格要求在任何时候只有一个writer。 Namenode全权管 理 block的复制，它周期性地从集群中的每个 Datanode接收心跳包和一个 Blockreport。心跳包的接收表示该 Datanode节点正常工 作，而 Blockreport包括了该 Datanode上所有的 block组成的列表 </p>
<ul>
<li><p>副本的存放，副本的存放是HDFS可靠性和性能的关键。 HDFS采用一种称为rack-aware的策略来改进数据的可靠性、有效性和网络带宽的利 用。这个策略实现的短期目标是验证在生产环境下的表现，观察它的行为，构建测试和研究的基础 ，以便实现更先进的策略。庞大的HDFS实例一般运行在多个机架的计算机形成的集群上，不同机架间的两台机器的通讯需要通过交换机，显然通常情况下，同一个机架内的两个节点间的带宽会比不同机架间的两台机器的带宽大。<br>  通过一个称为Rack Awareness的过程，Namenode决定了每个 Datanode所属的rack id。一个简单但没有优化的策略就是将副本存放在单独的机架上。这样可以防止整个机架（非副本存放）失效的情况，并且允许读数据的时候可以从多个机架读取。这个简单策略设置可以将副本分布在集群中，有利于组件失败情况下的负载均衡。但是，这个简单策略加大了写的代价，因为一个写操作需要传输block到多个机架。<br>  在大多数情况下， replication因子是 3， HDFS的 存放策略是将一个副本存放在本地机架上的节点，一个副本放在同一机架上的另一个节点，最后一 个副本放在不同机架上的一个节点。机架的错误远远比节点的错误少，这个策略不会影响到数据的可靠性和有效性。三分之一的副本在一个节点上，三分之二在一个 机架上，其他保存在剩下的机架中，这一策略改进了写的性能。        </p>
</li>
<li><p>副本的选择，为了降低整体的带宽消耗和读延时，HDFS会尽量让reader读最近的副本。如果在 reader的同一个机架上有一个副本，那么就读该副本。如果一个HDFS集群跨越多个数据中心，那么reader也将首先尝试读本地数据中心的副本。</p>
</li>
<li><p>SafeMode<br>Namenode启动后会进入一个称为SafeMode的特殊状态，处在这个状态的Namenode是不会进行数据块的复制的。Namenode从所有的Datanode接收心跳包和Blockreport。Blockreport包括了某个 Datanode所有的数据块列表。每个block都有指定的最小数目的副本。当Namenode检测确认某个 Datanode的数据块副本的最小数目，那么该Datanode就会被认为是安全的；如果一定百分比（这个参数可配置）的数据块检测确认是安全的，那么Namenode将退出SafeMode状态，接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些block复制到其他Datanode。</p>
</li>
</ul>
<h4 id="文件系统元数据的持久化">文件系统元数据的持久化</h4><p>Namenode存储 HDFS的元数据。对于任何对文件元数据产生修改的操作，Namenode都使用一个称为 Editlog的事务日志记录下来。例如，在HDFS中创建一个文件，Namenode就会在Editlog中插入一条记录来表示；同样，修改文件的replication因子也将往Editlog插入一条记录。Namenode在本地OS的文件系统中存储这个Editlog。整个文件系统的namespace，包括block到文件的映射、文件的属性，都存储在称为FsImage的文件中，这个文件也是放在Namenode所在系统的文件系统上。<br>    Namenode在内存中保存着整个文件系统namespace和文件Blockmap的映像。这个关键的元数据设计得很紧凑，因而一个带有 4G内存的 Namenode足够支撑海量的文件和目录。当Namenode启动时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用（apply)在内存中的 FsImage ，并将这个新版本的FsImage从内存中flush到硬盘上,然后再truncate这个旧的Editlog，因为这个旧的 Editlog的事务都已经作用在FsImage上了。这个过程称为checkpoint。在当前实现中， checkpoint只发生在 Namenode启动时，在不久的将来我们将实现支持周期性的checkpoint。<br>    Datanode并不知道关于文件的任何东西，除了将文件中的数据保存在本地的文件系统上。它把每个 HDFS数据块存储在本地文件系统上隔离的文件中。Datanode并不在同一个目录创建所有的文件，相反，它用启发式地方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录创建 所有的文件不是最优的选择，因为本地文件系统可能无法高效地在单一目录中支持大量的文件。当一个 Datanode启动时，它扫描本地文件系统，对这些本地 文件产生相应的一个所有HDFS数据块的列表，然后发送报告到Namenode，这个报告就是Blockreport。</p>
<h4 id="通讯协议">通讯协议</h4><p>   所有的 HDFS通讯协议都是构建在 TCP/IP协议上。客户端通过一个可配置的端口连接到 Namenode，通过 ClientProtocol与 Namenode交互。而 Datanode是使用 DatanodeProtocol与 Namenode交互。从 ClientProtocol和 Datanodeprotocol抽象出一个远程调用 (RPC），在设计上， Namenode不会主动发起 RPC，而是是响应来自客户端和 Datanode 的 RPC请求。</p>
<h4 id="健壮性">健壮性</h4><p>HDFS的主要目标就是实现在失败情况下的数据存储可靠性。常见的三种失败：Namenode failures, Datanode failures和网络分割（network partition s)。  </p>
<ul>
<li><p>硬盘数据错误、心跳检测和重新复制<br>  每个Datanode节点都向Namenode周期性地发送心跳包。网络切割可能导致一部分Datanode跟 Namenode失去联系。 Namenode通过心跳包的缺失检测到这一情况，并将这些Datanode标记为 dead，不会将新的IO请求发给它们。寄存在dead Datanode上的任何数据将不再有效。Datanode的死亡可能引起一些block的副本数目低于指定值，Namenode不断地跟踪需要复制的block，在任何需要的情况下启动复制。在下列情况可能需要重新复制：某个 Datanode节点失效，某个副本遭到损坏，Datanode上的硬盘错误，或者文件的replication因子增大。</p>
</li>
<li><p>集群均衡<br> HDFS支持数据的均衡计划，如果某个Datanode节点上的空闲空间低于特定的临界点，那么就会启动一个计划自动地将数据从一个Datanode搬移到空闲的Datanode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并分布到集群中以满足应用的要求。这些均衡计划目前还没有实现。</p>
</li>
<li><p>数据完整性<br>从某个Datanode获取的数据块有可能是损坏的，这个损坏可能是由于Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了HDFS文件内容的校验和。当某个客户端创建一个新的 HDFS文件，会计算这个文件每个block的校验和，并作为一个单独的隐藏文件保存这些校验和在同一个 HDFS namespace下。当客户端检索文件内容，它会确认从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode获取该block的副本。</p>
</li>
<li><p>元数据磁盘错误<br>  FsImage和Editlog是HDFS的核心数据结构。这些文件如果损坏了，整个HDFS实例都将失效。因而，Namenode可以配置成支持维护多个FsImage和Editlog的拷贝。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这个同步操作可能会降低Namenode每秒能支持处理的namespace事务。这个代价是可以接受的，因为HDFS是数据密集的，而非元数据密集。当Namenode重启的时候，它总是选取最近的一致的FsImage和Editlog使用。<br> Namenode在HDFS是单点存在，如果Namenode所在的机器错误，手工的干预是必须的。目前，在另一台机器上重启因故障而停止服务的Namenode这个功能还没实现。</p>
</li>
<li><p>快照<br> 快照支持某个时间的数据拷贝，当 HDFS数据损坏的时候，可以恢复到过去一个已知正确的时间点。 HDFS目前还不支持快照功能。</p>
</li>
</ul>
<h4 id="数据组织">数据组织</h4><ul>
<li>数据块<br>  兼容HDFS的应用都是处理大数据集合的。这些应用都是写数据一次，读却是一次到多次，并且读的速度要满足流式读。HDFS支持文件的write-once-read-many语义。一个典型的block大小是64MB，因而，文件总是按照64M切分成chunk，每个chunk存储于不同的Datanode</li>
<li><p>步骤<br>  某个客户端创建文件的请求其实并没有立即发给Namenode，事实上，HDFS客户端会将文件数据缓存到本地的一个临时文件。应用的写被透明地重定向到这个临时文件。当这个临时文件累积的数据超过一个 block的大小（默认64M)，客户端才会联系Namenode。Namenode将文件名插入文件系统的层次结构中，并且分配一个数据块给它，然后返回Datanode的标识符和目标数据块给客户端。客户端将本地临时文件flush到指定的Datanode上。当文件关闭时，在临时文件中剩余的没有flush的数据也会传输到指定的 Datanode，然后客户端告诉Namenode文件已经关闭。此时Namenode才将文件创建操作提交到持久存储。如果Namenode在文件关闭前挂了，该文件将丢失。<br> 上述方法是对通过对HDFS上运行的目标应用认真考虑的结果。如果不采用客户端缓存，由于网络速度和网络堵塞会对吞估量造成比较大的影响。</p>
</li>
<li><p>流水线复制<br>  当某个客户端向HDFS文件写数据的时候，一开始是写入本地临时文件，假设该文件的 replication因子设置为 3，那么客户端会从Namenode获取一张Datanode列表来存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分（4kb)地接收数据，将每个部分写入本地仓库，并且同时传输该部分到第二个Datanode节点。第二个Datanode也是这样，边收边传，一小部分一小部分地收，存储在本地仓库，同时传给第三个Datanode，第三个Datanode就仅仅是接收并存储了。这就是流水线式的复制。</p>
</li>
</ul>
<h4 id="可访问性">可访问性</h4><p>   HDFS给应用提供了多种访问方式，可以通过DFSShell通过命令行与HDFS数据进行交互，可以通过 java API调用，也可以通过C语言的封装API访问，并且提供了浏览器访问的方式。正在开发通过 WebDav协议访问的方式。具体使用参考文档。</p>
<h4 id="空间的回收">空间的回收</h4><ul>
<li><p><strong>文件的删除和恢复</strong><br>  用户或者应用删除某个文件，这个文件并没有立刻从HDFS中删除。相反，HDFS将这个文件重命名，并转移到 /trash目录。当文件还在 /trash目录时，该文件可以被迅速地恢复。文件在 /trash中保存的时间是可配置的，当超过这个时间，Namenode就会将该文件从namespace中删除。文件的删除，也将释放关联该文件的数据块。注意到，在文件被用户删除和HDFS空闲空间的增加之间会有一个等待时间延迟。<br>  当被删除的文件还保留在 /trash目录中的时候，如果用户想恢复这个文件，可以检索浏览 /trash目录并检索该文件。 /trash目录仅仅保存被删除文件的最近一次拷贝。 /trash目录与其他文件目录没有什么不同，除了一点：HDFS在该目录上应用了一个特殊的策略来自动删除文件，目前的默认策略是 删除保留超过 6小时的文件，这个策略以后会定义成可配置的接口。</p>
</li>
<li><p><strong>Replication因子的减小</strong><br>  当某个文件的 replication因子减小， Namenode会选择要删除的过剩的副本。下次心跳检测就将该信息传递给 Datanode， Datanode就会移除相应的 block并释放空间，同样，在调用 setReplication方法和集群中的空闲空间增加之间会有一个时间延迟。</p>
</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-HDFS复制因子与块操作" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/05/HDFS复制因子与块操作/" class="article-date">
  	<time datetime="2015-11-05T07:41:40.000Z" itemprop="datePublished">2015-11-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/05/HDFS复制因子与块操作/">HDFS复制因子与块操作</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="关于复制因子">关于复制因子</h3><p>HDFS文件系统中文件存储的最小单位是块，块的大小是可以在配置文件中配置的，一般配置为64M或者128M,这就意味着哪怕一个文件只有1K,也会占用集群实际硬盘128M的硬盘，这种设计的初衷是为了尽可能的减小维护数据目录的成本，增快数据查找,归并的速度。从另一方面也要求我们在存入数据时尽量减少零碎数据的产生。<br>HDFS可以通过<code>replication</code>这个参数来改变集群中的复制因子。    </p>
<p><strong>注意</strong> ：这里的复制因子指的是系统中文件块的副本数，注意这里指的是块的副本数而不是文件的副本数。</p>
<p>我们可以通过在配置文件中修改参数修改复制因子，但要<code>注意</code>修改复制因子重启HDFS后，以前的数据复制因子不会改变，因为在HDFS中复制因子这个参数是赋予到每个文件中的，之前已经put进hdfs中的文件的复制因子会保留在当时集群设置的复制因子值。当然你也可以在put某个文件时单独指定它的复制因子。如：</p>
<pre><code>hdfs dfs -D dfs.replication=<span class="number">1</span> -put <span class="number">70</span>M logs/<span class="number">2</span>
</code></pre><p>对正式环境集群，需要统一整个集群所有文件的复制因子。可以执行以下命令</p>
<pre><code>hadoop fs -setrep -R <span class="number">3</span> /
</code></pre><p>这样根目录<code>/</code>以下的所有文件的块都会复制为3.hadoop官方默认的replication参数就是3.这也是hadoop正式集群要求datanode至少为3个的原因。<br>而默认的分布是至少有1份在另一个datanode上，如果对机器进行了机架的分配则分不规则是A一份，与A同机架的B一份，不同机架的C一份。这样最高的保证了数据不会物理丢失。</p>
<h3 id="关于块的检查与操作。">关于块的检查与操作。</h3><p>fsck HDFS文件系统检查工具。   </p>
<pre><code>hdfs fsck <span class="comment">[GENERIC_OPTIONS]</span> &lt;path&gt; <span class="comment">[-move | -delete | -openforwrite]</span> <span class="comment">[-files <span class="comment">[-blocks <span class="comment">[-locations | -racks]</span>]</span>]</span>
</code></pre><table>
<thead>
<tr>
<th>命令选项</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>检查的起始目录。</td>
</tr>
<tr>
<td>-move</td>
<td>移动受损文件到/lost+found</td>
</tr>
<tr>
<td>-delete</td>
<td>删除受损文件。</td>
</tr>
<tr>
<td>-openforwrite</td>
<td>打印出写打开的文件。</td>
</tr>
<tr>
<td>-files</td>
<td>打印出正被检查的文件。</td>
</tr>
<tr>
<td>-blocks</td>
<td>打印出块信息报告。</td>
</tr>
<tr>
<td>-locations</td>
<td>打印出每个块的位置信息。</td>
</tr>
<tr>
<td>-racks</td>
<td>打印出data-node的网络拓扑结构。 </td>
</tr>
</tbody>
</table>
<p>以上检查出 整个文件系统的块信息</p>
<p><img src="http://bcygamer.wang/img/fsckcheck.png" alt=""></p>
<pre><code>/user/hive/warehouse/app1001/log_track/actionsub=install/datesub=2015-11-01/events-.1446463158815: MISSING 1 blocks of total size 131 B...../user/hive/warehouse/app1001/log_track/actionsub=install/datesub=2015-11-02/.events-.1446466681766.tmplog 133 bytes, 1 block(s), OPENFORWRITE: ........../user/hive/warehouse/app1001/log_track/actionsub=install/datesub=2015-11-02/events-.1446462338729 133 bytes, 1 block(s), OPENFORWRITE:
/user/hive/warehouse/app1001/log_track/actionsub=install/datesub=2015-11-02/events-.1446462338729: MISSING 1 blocks of total size 133 B
....................................................................................................
...................................................Status: CORRUPT
Total size:    637720798464 B
Total dirs:    8270
Total files:   68551
Total symlinks:                0
Total blocks (validated):      69016 (avg. block size 9240187 B)
<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>
CORRUPT FILES:        61
MISSING BLOCKS:       61
MISSING SIZE:         2332908 B
<span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span>
Minimally replicated blocks:   68955 (99.91161 %)
Over-replicated blocks:        0 (0.0 %)
Under-replicated blocks:       119 (0.17242378 %)
Mis-replicated blocks:         0 (0.0 %)
Default replication factor:    3
Average block replication:     2.9935522
Corrupt blocks:                0
Missing replicas:              238 (0.11496252 %)
Number of data-nodes:          4
Number of racks:               1
FSCK ended at Wed Nov 04 06:01:15 UTC 2015 in 1285 milliseconds

The filesystem under path '/' is CORRUPT
</code></pre><p>从上面信息中可以得到丢失的块。一般情况下，HDFS的系统是很稳健，不会出现全部副本丢失的情况。假设出现全部副本丢失，我们只能删除这些块所属的文件以保证系统可以正常运行，将损失减到最低。</p>
<h3 id="增删DataNode时块副本同步机制的作用。">增删DataNode时块副本同步机制的作用。</h3><p>新增节点可以使用平衡数据命令来平衡数据。<br>删除节点需要一个一个的删除，并且删除一个之后，注意检查将块补充成3份，再删除另一个。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-NameNode数据目录丢失" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/04/NameNode数据目录丢失/" class="article-date">
  	<time datetime="2015-11-04T09:19:05.000Z" itemprop="datePublished">2015-11-04</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/04/NameNode数据目录丢失/">NameNode数据目录丢失</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="NameNode目录丢失">NameNode目录丢失</h1><h3 id="起因">起因</h3><p>这是一个很惨痛的经历，公司数据2T的数据量，游戏用户行为数据分析系统处于内测阶段。采用ClouderaManager管理CDH各个组件。<br>今天意图使用HA高可用。  </p>
<p>这里的具体细节不再论述，总之是高可用双namenode部署失败了，返回单节点NameNode，需要对namenode进行format,而用过CM的用户知道，它是不支持namenode目录不为空时的format的。<br>这里一不小心吧namenode中的目录日志nn删除了。<br><strong>在敲下enter的瞬间，脑子就热了。。擦！</strong></p>
<p>果不其然，启动后数据都不可见了。只能看到flume刚实时导入的数据。</p>
<h3 id="思考">思考</h3><p>OK，吃饭时间到了，先吃晚饭，之后查了下解决方案。第一点无意是尝试恢复linux下被删除的数据。无奈hadoop集群重启后硬盘已经被写入了。恢复硬盘数据已经不简单了。<br>在不了解一下的解决方案前，思考Hadoop 会不会有扫描块之后重建namenode目录的实现方案。<br>无奈，没有知道相关的文章和文档。 找到了以下的解决方案。</p>
<h3 id="解决">解决</h3><h4 id="解决思路原理">解决思路原理</h4><p>在解决这个问题之前，我们需要先说一下HDFS文件系统维护中的几个角色。   </p>
<ul>
<li><strong>NameNode</strong></li>
<li><strong>DataNode</strong></li>
<li><strong>SecondaryNameNode</strong></li>
</ul>
<p>这里我们不介绍MapReduce的过程，所以对DataNode的理解就单纯的理解为储存数据块的节点。<br>在一个HDFS文件系统中，如果不启用HA,那么NameNode就是这个这个文件系统的核心管理器也是单节点的瓶颈所在，我们先看一下NameNode对DataNode中的数据块的维护方式。<br><img src="http://bcygamer.wang/img/namenodedir.png" alt="NameNode目录下的内容"><br><strong><code>提前说明</code></strong>  NameNode 对数据块的维护不是对定时对目录树进行snapshot，而是类似于一种追加的模式。每次NameNode重启时会将所有的追加日志进行一次replay,以恢复HDFS目录树最终的状态。   </p>
<p>#####下面对目录下的文件进行一下说明:</p>
<ul>
<li><strong>edits_</strong>: edits文件存放的是Hadoop文件系统的所有更新操作的路径，文件系统客户端执行的所以写操作首先会被记录到edits文件中。</li>
<li><strong>fsimage_</strong> : 文件其实是Hadoop文件系统元数据的一个永久性的检查点，其中包含Hadoop文件系统中的所有目录和文件idnode的序列化信息。</li>
<li><strong>seen_txid</strong> : 它代表的是edits_文件的尾号数，format之后是这个transactionId为0，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯。<br><code>fsimage</code>和<code>edits</code>文件都是经过序列化的，在NameNode启动的时候，它会将fsimage文件中的内容加载到内存中，之后再执行edits文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。</li>
</ul>
<h3 id="Question"><code>Question</code></h3><p>我们现在丢失了NameNode下面所有目录数，导致NameNode重启之后再也找不到HDFS中的块。这就是崩溃性的灾难。<br>现在我们来说一下<code>SecondaryNameNode</code>（简称snn）在HDFS系统中的作用。<br><img src="http://bcygamer.wang/img/fsimage_edits.png" alt="ssn实现"><br>snn会周期性德将<code>edits</code>操作记录合并到一个checkpoint中，即fsimage，然后清空editlog.所以nn每次重启都会load最近的一个fsimage,然后replay editlog中的hdfs操作。如果没有snn这个周期性的合并过程，每次namenode重启会需要replay所有的editlog，这是一个相当大的工作量，会花费很长时间。<br>而这样周期性的合并就能减少重启的时间。同时也能保证HDFS系统的完整性。<br>这就是SecondaryNameNode所做的事情。</p>
<h3 id="Solution"><code>Solution</code></h3><p>在snn目录节点下备份有nn目录，在我们丢失了nn目录的情况下可以手动将snn节点下的current目录cp到nn目录下。然后重启nn,即可解决。所以一般注意将snn目录和n目录分布在不同的节点上。 这里注意一点snn目录下的数据未必一定是最新的数据，可以比较seen_txid查看一下，或者重启之后查看下hdfs系统中的丢失块，争取将损失减到最小。下一篇文章会对hdfs中的<code>复制因子</code>和<code>块操作</code>做一些介绍。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-Compass项目架构变化" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/09/08/Compass项目架构变化/" class="article-date">
  	<time datetime="2015-09-08T03:43:37.000Z" itemprop="datePublished">2015-09-08</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/08/Compass项目架构变化/">Compass项目架构变化</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="前版本">前版本</h3><p><strong>roles</strong>:</p>
<ul>
<li>trackserver        </li>
<li>master</li>
</ul>
<p><strong>处理日志脚本</strong></p>
<p>trackserver生成log-[m1-9]-date文件名的日志文件，定时执行batchHiveData.js，处理后日期的生成log-hive-date文件，过期日志放在log-out/目录下。</p>
<p><strong>load日志脚本</strong></p>
<p>master在处理日志脚本执行完成后，通过batch-load-hive-data.sh.<br>scp log-hive-date 及log-out/的数据到master主机，并按照正常数据和过期的数据的区别按日期load到hive中。<br>之后，用sqoop，load-user-data.sh将mysql中的user相关的快照表load到hive中。</p>
<h3 id="新版本">新版本</h3><p><strong>roles</strong>  </p>
<ul>
<li>trackserver</li>
<li>flume-agent</li>
<li><p>flume-collect</p>
<p>trackserver 产生log-[1-9] 文件名的符合hive需求的文件，flume-agent 通过execSource ，用tailf这些文件变化将log日志传输到flume-collect,使用hdfssinks 将日志流式存入hdfs中。然后依据数据格式建立不同的hive表结构进行数据查询。</p>
<p><strong>注意点</strong></p>
</li>
</ul>
<ol>
<li><p>因为公司数据项目繁多，需要按照action和date对hive进行分区，我们hive采用外部表的方式，所以需要对log日志进行处理，我使用regex_extractor，将log日志的action和time抽取出来，放到head中，通过events传到sinks中。注意：我们的log日志中记录的是10位的timestamp，而flume中的时间占位符识别的是13位的timestamp，所以我对日志中的timestamp进行了*1000的处理。进而通过actionsub做一级分区，而datesub做二级分区。</p>
</li>
<li><p>因为hive外部表的元数据分区信息需要受到插入，否则哪怕location到准确目录，也是读取不到数据的。我采用的方案是采用定时任务，每天执行一次批量插入当前日期前后7天的分区信息。</p>
</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-flume搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/08/12/flume搭建/" class="article-date">
  	<time datetime="2015-08-12T09:39:51.000Z" itemprop="datePublished">2015-08-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/12/flume搭建/">flume搭建</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>游戏公司需要分析游戏用户的所有行为，现在的项目是一个消除类的休闲游戏，需要记录分析用户的每一次拉框操作。在游戏内测期间，为了快速开发，直接简单粗暴的用了mysql来记录。如今北美用户量是200W左右，每天log的数量在亿量级。so,mysql been gived up!</p>
<p>第一阶段，我使用了hadoop+hive+sqoop2的架构来简单粗暴的搭建了一套日志生成处理分析的BI系统，懂行的人知道这显然是很low的！so，这里介绍下<a href="https://flume.apache.org/" target="_blank" rel="external">flume</a>得使用。</p>
<p><strong>简介</strong><br>flume是apache旗下的一套日志收集系统，现在大家使用的一般是Flume NG的版本<br>，只有一种角色就是agent。作为配置即可用的组件，有三个内容需要配置：</p>
<ul>
<li>source 作为数据来源</li>
<li>sink 处理数据去向</li>
<li>channel 连接source和sink之间的通道</li>
</ul>
<p><strong>安装</strong></p>
<p>flume显然是可以用ClouderaManager来管理安装。但是现实情况是我们不需要将所有的flume agent主机纳入Cloudera的管理中。所以可以下载包之后直接<code>tar xzvf ...</code> ,需要配置一下FLUME_HOME 和JDK的PATH。</p>
<p><strong>配置</strong></p>
<p>首先来一个官网上的demo</p>
<pre><code># example.conf: A single-node Flume configuration
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# <span class="operator"><span class="keyword">Describe</span>/configure the <span class="keyword">source</span>
a1.sources.r1.<span class="keyword">type</span> = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = <span class="number">44444</span>

# <span class="keyword">Describe</span> the sink
a1.sinks.k1.<span class="keyword">type</span> = logger

# <span class="keyword">Use</span> a channel which buffers <span class="keyword">events</span> <span class="keyword">in</span> <span class="keyword">memory</span>
a1.channels.c1.<span class="keyword">type</span> = <span class="keyword">memory</span>
a1.channels.c1.<span class="keyword">capacity</span> = <span class="number">1000</span>
a1.channels.c1.transactionCapacity = <span class="number">100</span>

# Bind the <span class="keyword">source</span> <span class="keyword">and</span> sink <span class="keyword">to</span> the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1</span>
</code></pre><h3 id="Source">Source</h3><ol>
<li><code>avro</code> 用于监听端口采集数据(采用avro通信协议传输),一般agent之间传输时可以使用。</li>
<li><code>Thrift</code> 用于监听端口采集数据(采用Thrift通信协议传输)，同上</li>
<li><p><code>exec</code> 执行shell脚本来采集数据，一般使用：</p>
<p> <code>tail -F [file]</code></p>
<p> <code>a1.sources.r1.command = tail -F /var/log/secure</code></p>
<p> 支持对命令或者脚本的重启</p>
</li>
<li><code>jms</code> 从消息队列中获取消息</li>
<li><code>netcat</code> 监控指定端口，每一行作为一个event传输。</li>
<li><code>http</code> 支持http的post和get,需要指定handler</li>
<li><code>Scribe</code> 主要用于兼容Scribe.</li>
<li><code>syslog</code> 监听syslog，支持tcp和udp，支持多端口监听</li>
<li><code>sequence</code> 用于测试，产生自增编号的event</li>
<li><code>spooldir</code> 监控某个目录下的所有文件，将其新加入的文件作为数据源传输走；<br>每传输玩一个文件后，会被rename成其他名字（表示已经传输过）或者删掉；<br>默认监控目录下的文件具有：immutable、uniquely-named属性，否则会出错；</li>
<li><code>kafka</code> 获取kafka产生的的数据（可能后面会采用的架构）</li>
</ol>
<h3 id="Channels">Channels</h3><ol>
<li><code>memory channel</code> 消息放在内存中，提高吞吐，但不可靠，可能丢失数据</li>
<li><code>JDBC channel</code> 内置的derb数据库，对event进行了持久化，提供高可靠性；<br>看来是取代同样具有持久特性的file channel</li>
<li><code>Kafka Channel</code> 结合kafka使用</li>
<li><code>File</code> 会对数据进行持久化，保证数据安全，有损性能</li>
<li><code>SPILLABLEMEMORY</code> 类似与memory和file的折中方法，优先使用memory,剩下的就都持久化到file中等待</li>
</ol>
<h5 id="Channel_Selectors">Channel Selectors</h5><p>这里介绍一下这个<br>这里的channel Selectors主要用来处理一个source有两个及以上的的sink的时候，需要通过不同的channel来发送，这个时候就需要channel selectors .</p>
<p>官方文档上channel selectors 有两种类型:      </p>
<p>Replicating Channel Selector (default)</p>
<p>Multiplexing Channel Selector<br>这里具体是设置可以参照<a href="http://blog.csdn.net/xiao_jun_0820/article/details/38116103" target="_blank" rel="external">这篇博客</a></p>
<h3 id="Sinks">Sinks</h3><p>sinks，负责处理source来得数据。sink的种类也有很多<br>hdfs,hive,logger,Avro,Thrift,IRC,file_roll,null,Hbase,asynchbase,MorphlineSolrSink,ElasticSearchSink,Kite Dataset,Kafka.</p>
<p>因为这里没有一个个使用过，大家可以去<a href="https://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="external">flume</a>官方文档上查看。我自己用的应该是Hive,所以后面应该会写一篇Hive作为sinks的详尽的文章。</p>
<p>下面给出一个测试环境的配置代码<br>测试环境有2台主机，track主机会产生日志文件储存在文件中，core主机是中心节点，用于收集数据并输出出来。</p>
<p><strong>track主机</strong></p>
<p> 文件名：flume-track.conf</p>
<pre><code><span class="title">agent</span>.sources = r1
<span class="title">agent</span>.channels = c1
<span class="title">agent</span>.sinks = s1

#<span class="type">For</span> each one <span class="keyword">of</span> the sources, the <span class="typedef"><span class="keyword">type</span> is defined</span>
<span class="title">agent</span>.sources.r1.<span class="typedef"><span class="keyword">type</span> = exec</span>

<span class="title">agent</span>.sources.r1.command = tail -f /data/track_diagon/data/log-<span class="number">9</span>-<span class="number">2015</span>-<span class="number">08</span>-<span class="number">11</span>

# <span class="type">Each</span> sink's <span class="typedef"><span class="keyword">type</span> must be defined</span>
<span class="title">agent</span>.sinks.s1.<span class="typedef"><span class="keyword">type</span> = avro</span>
<span class="title">agent</span>.sinks.s1.hostname = <span class="number">172.31</span><span class="number">.30</span><span class="number">.204</span>
<span class="title">agent</span>.sinks.s1.<span class="foreign"><span class="keyword">port</span> = 60000</span>

# <span class="type">Each</span> channel's <span class="typedef"><span class="keyword">type</span> is defined.</span>
<span class="title">agent</span>.channels.c1.<span class="typedef"><span class="keyword">type</span> = memory</span>
<span class="title">agent</span>.channels.c1.capacity = <span class="number">100</span>
# <span class="type">Other</span> config values specific to each <span class="typedef"><span class="keyword">type</span> of channel<span class="container">(<span class="title">sink</span> <span class="title">or</span> <span class="title">source</span>)</span></span>
# can be defined <span class="keyword">as</span> well
<span class="title">agent</span>.sources.r1.channels = c1
<span class="title">agent</span>.sinks.s1.channel = c1
</code></pre><p><strong>core主机</strong></p>
<p>文件名：flume-core.conf</p>
<pre><code><span class="title">agent</span>.sources = r1
<span class="title">agent</span>.channels = c1
<span class="title">agent</span>.sinks = s1

# <span class="type">For</span> each one <span class="keyword">of</span> the sources, the <span class="typedef"><span class="keyword">type</span> is defined</span>
<span class="title">agent</span>.sources.r1.<span class="typedef"><span class="keyword">type</span> = avro</span>
<span class="title">agent</span>.sources.r1.bind = <span class="number">172.31</span><span class="number">.30</span><span class="number">.204</span>
<span class="title">agent</span>.sources.r1.<span class="foreign"><span class="keyword">port</span> = 60000</span>

# <span class="type">Each</span> sink's <span class="typedef"><span class="keyword">type</span> must be defined</span>
<span class="title">agent</span>.sinks.s1.<span class="typedef"><span class="keyword">type</span> = logger</span>

# <span class="type">Each</span> channel's <span class="typedef"><span class="keyword">type</span> is defined.</span>
<span class="title">agent</span>.channels.memoryChannel.<span class="typedef"><span class="keyword">type</span> = memory</span>
<span class="title">agent</span>.channels.memoryChannel.capacity = <span class="number">1000</span>
# <span class="type">Other</span> config values specific to each <span class="typedef"><span class="keyword">type</span> of channel<span class="container">(<span class="title">sink</span> <span class="title">or</span> <span class="title">source</span>)</span></span>
# can be defined <span class="keyword">as</span> well
<span class="title">agent</span>.sources.r1.channels = c1
<span class="title">agent</span>.sinks.s1.channel = c1
</code></pre><p>然后先在core主机的flume文件价下运行：</p>
<pre><code>bin/flume-ng  agent -c <span class="keyword">conf</span> -f <span class="keyword">conf</span>/flume-core.<span class="keyword">conf</span> -<span class="keyword">n</span> agent 
</code></pre><p>然后在track主机上运行：</p>
<pre><code>bin/flume-ng  agent -c conf -f conf/flume-track<span class="class">.conf</span> -n agent -Dflume<span class="class">.root</span><span class="class">.logger</span>=INFO,console
</code></pre><p>这样如果track主机的日志文件内容增加，core主机就会输出日志文件所生成的event!</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-ClouderaManager入门使用" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/08/11/ClouderaManager入门使用/" class="article-date">
  	<time datetime="2015-08-11T09:43:19.000Z" itemprop="datePublished">2015-08-11</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/11/ClouderaManager入门使用/">ClouderaManager入门使用</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>  因为工作原因，自己搭建Hadoop生态环境，包括hdfs，hive,sqoop2,flume在第一阶段的摸索时，使用apache系列原生的包来安装，配置，使用。磕磕绊绊花了接近半个月将整套东西应用部署到生产环境上。闲暇之后仔细研究下了各种配置，之后看到了2大神器分别是：<a href="https://ambari.apache.org/" target="_blank" rel="external">Ambari</a>和<a href="http://www.cloudera.com/content/cloudera/zh-CN/documentation/core/v5-3-x/topics/cm_ig_install_path_a.html" target="_blank" rel="external">ClouderaManager</a>.<br>  对于Ambari,开始前虽然看到官网上说明了Ubuntu系统只支持到12.不过本着试试看看的心态研究了1天，环境搭好之后，安装<code>HDP</code>的时候系统检测那一关果然是过不去。OK，本着不跟自己过不去的原则，give up it!</p>
<p>  ClouderaManager的使用真心让我很惊喜。</p>
<h3 id="安装ClouderaManager"><strong>安装ClouderaManager</strong></h3><p><strong>这里以Ubuntu14.04系统为例</strong></p>
<p>本人很懒，Couldera显然也很懒，这个发行版就是给懒人准备的嘛！<br>所以显然使用自动安装最好 </p>
<p>1.直接在首页下载cloudera-manager-installer.bin文件 </p>
<p>2.授予可执行权限    </p>
<pre><code><span class="variable">$ </span>chmod u+x cloudera-manager-installer.bin
</code></pre><p> 3.直接sudo运行安装，这里有一个地方需要注意一下。因为Cloudera如果安装失败了卸载过程非常繁杂，所以为了避免因为断网，终端安装的是发生，可以使用screen 来安装。</p>
<pre><code><span class="variable">$ </span>sudo ./cloudera-manager-installer.bin
</code></pre><p> 安装过程中会自动安装jdk7，所以你不要预先安装。</p>
<p> 4.安装道路就是一系列的yes,ok。。安装成功后cloudera-server会自动启动，并监听<code>7180</code>端口<br> 之后就可以访问了，第一次默认登录账号是admin/admin.</p>
<p> 这里有一个地方需要注意，似乎机器性能太差的机器是无法正常运行的。我没有在官网上发现这个说法，但是我使用2核4G的机器做实验安装成功之后，启动server之后过几秒就回dead。</p>
<h3 id="安装Hadoop集群"><strong>安装Hadoop集群</strong></h3><p>在安装之前有2件事是需要大家做的。</p>
<ul>
<li>设置每台你要加入集群中的机器之间的ssh链接</li>
<li>就是在每主机的<code>/etc/hosts</code>文件里加入集群中的每台主机的内网IP和主机名。这样直接填进去搜索就可以了。</li>
</ul>
<p>以admin/admin登录界面之后，是选择主机，之后选择CDH的版本，这里一般选择默认就可以了。</p>
<p>在ssh的界面，需要指定你的cloudera-server所在机器的密钥文件。并且注定用户</p>
<p>后面就是选择hadoop服务的安装。</p>
<p>在这里有几个地方需要注意：</p>
<ul>
<li>首先Hive的元数据服务可能需要制定mysql等，这里需要在主机上<code>安装mysql的jdbc driver</code><br>,执行：</li>
</ul>
<pre><code>sudo apt-get <span class="operator"><span class="keyword">install</span> libmysql-<span class="keyword">java</span></span>
</code></pre><p>否在会在启动服务时报错误。</p>
<ul>
<li>另外一个问题就是关于cloudera5 里新出现的单用户模式，据官方文档上说是可以用单个用户执行所有的服务，但是要使用这个单个用户模式会涉及到很多的用户权限问题，我自己就在这个问题上坑了很久。</li>
</ul>
<p>后来经过实践发现可以按照<a href="http://www.cloudera.com/content/cloudera/zh-CN/documentation/core/v5-3-x/topics/install_singleuser_reqts.html?scroll=xd_583c10bfdbd326ba--69adf108-1492ec0ce48--7ade__section_n53_hd1_fq" target="_blank" rel="external">官网上</a>的说明操作，<code>注意：</code>我们采用的自动安装模式不需要进行B和C中的设置。即只操作：</p>
<pre><code>usermod -<span class="tag">a</span> -G sudo cloudera-scm
</code></pre><p>取消注释：/etc/pam.d/su 里，session required pam_limits.so。</p>
<p>其他操作则不用进行。</p>
<h3 id="技巧"><strong>技巧</strong></h3><p>经过趟坑，发现有以下几点可以避免权限错误。</p>
<ul>
<li><p>我们可以挂在一块单独的硬盘，挂载到根目录下得/data目录上，之后讲安装集群过程中所有的内容都知道这个data上，即将之前的/var/log,/var/lib目录的内容都制定到/data/cm/var/log,/data/cm/var/lib里面。并且将/data/cm的目录权限授予cloudera-scm用户。</p>
<pre><code>sudo chown cloudera-<span class="string">scm:</span>root data/
sudo chown cloudera-<span class="string">scm:</span>root cm/
</code></pre></li>
</ul>
<p>注意这里的组权限必须是root而不能是cloudera-scm，否则启动服务时会警告，虽然我也不清楚是为什么。</p>
<p>如果以上的过程你都经历了，那么恭喜你，终于不要满世界的机器去看状态和改配置文件了（大公司的哥们忽略。。）！</p>
<p>自己也刚刚开始使用，后期会继续摸索。。。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-mysql忘记密码处理" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/08/10/mysql忘记密码处理/" class="article-date">
  	<time datetime="2015-08-10T09:31:23.000Z" itemprop="datePublished">2015-08-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/10/mysql忘记密码处理/">mysql忘记密码处理</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>工作中会使用很多的主机，偶尔用到一台很久不用的主机，需要mysql的服务，作为实验环境第一反应会去apt-get intsall mysql-server. 可是发现已经安装过。 then,what’s the password for root?</p>
<p>  <strong>进入免权限认证处理</strong></p>
<p>  <code>sudo vim /etc/mysql/my.cnf</code></p>
<p>  把<code>skip-grant-tables</code> 在[mysqld]段中。</p>
<p>  重启<code>mysql sudo service mysql restart</code></p>
<p>  然后可以直接无密码登录mysql,</p>
<pre><code>mysql -uroot;

<span class="operator"><span class="keyword">use</span> mysql;</span>

<span class="operator"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span> = <span class="keyword">password</span>(<span class="string">'new password'</span>) <span class="keyword">where</span> <span class="keyword">user</span> = <span class="string">'root'</span>;</span>

<span class="operator"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span>

exit;
</code></pre><p>然后把之前修改的my.cnf文件改回来，之后重启MySQL服务。</p>
<p>即修改密码成功</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mysql/">mysql</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-mysql新建用户权限控制" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/08/10/mysql新建用户权限控制/" class="article-date">
  	<time datetime="2015-08-10T09:12:37.000Z" itemprop="datePublished">2015-08-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/10/mysql新建用户权限控制/">mysql新建用户权限控制</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>创建用户</strong>  </p>
<p><code>create user username identified by &#39;password&#39;</code></p>
<p><strong>授权</strong></p>
<p><code>grant all privileges on databasename.* to &#39;username&#39;@&#39;localhost&#39; identified by &#39;password&#39;</code>  </p>
<p>这里如果需要远程登录则将localhost换成需要的主机名，如果是任意地点登录则为<code>%</code>， 有一点需要注意，需要在    <code>/etc/mysql/my.cnf</code> 中将</p>
<p><code>bind-address = 127.0.0.1</code> 注释掉。</p>
<h3 id="然后刷新权限">然后刷新权限</h3><p><code>flush privileges</code></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mysql/">mysql</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
    <article id="post-开站序言" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/08/03/开站序言/" class="article-date">
  	<time datetime="2015-08-03T14:31:43.000Z" itemprop="datePublished">2015-08-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/03/开站序言/">开站序言</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="序言">序言</h3><ol>
<li>本人游戏公司全栈工程师一名，自己学习的东西越来越繁杂，很多琐碎的东西挤一起来很容易忘记，所以找寻了一下考虑建一个技术博客，用于记录学习的过程，也本着分享的精神，给跟我遇到同样问题的同学一个思路。额，共勉！</li>
<li>额，整理一下技术方向，首先语言方面个人比较擅长JS和JAVA，所以一些文章基本都是基于此来说明的。首先呢，我现在处于一个创业公司，公司的hadoop生态系统是又我负责搭建的，所以近期可能会偏这方面的各个组件的内容偏多。然后是游戏服务器架构方面以及代码级别的一些内容，都会慢慢的讲道。需要的同学可以联系我沟通。</li>
<li><p>在记录技术生活的同时，自己也会更新一些不实名的杂记等，可能未来会加密，但前期应该是开放的。</p>
<p> 希望我可以持续的发展我的站点。另外，感谢这个域名的妹子！</p>
</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/随笔/">随笔</a></li></ul>
	</div>

      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>







  
  
</div>
            <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2015 Gamer Bao
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
        <span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>
<span id="busuanzi_container_page_pv">
  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</span>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>